<!DOCTYPE html>
<link rel="stylesheet" href="https://pages.nist.gov/nist-header-footer/css/nist-combined.css">
      <script src="https://pages.nist.gov/nist-header-footer/js/jquery-1.9.0.min.js" type="text/javascript" defer="defer"></script>
      <script src="https://pages.nist.gov/nist-header-footer/js/nist-header-footer.js" type="text/javascript" defer="defer"></script>


<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">

		<link rel="stylesheet" href="/RMF/css/screen.css">
		<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Merriweather:400italic,400,300italic,300,700,700italic|Open+Sans:400italic,600italic,700italic,700,600,400|Inconsolata:400,700">

		

		<!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Search | AI RMF Playbook</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Search" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/RMF/search.html" />
<meta property="og:url" content="http://localhost:4000/RMF/search.html" />
<meta property="og:site_name" content="AI RMF Playbook" />
<script type="application/ld+json">
{"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/RMF/NISTlogo.png"}},"headline":"Search","@type":"WebPage","url":"http://localhost:4000/RMF/search.html","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

		<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/RMF/feed.xml" title="AI RMF Playbook" />

	</head>

	<body class="">
		<header>
			<div class="wrapper">
				<section class="top-bar">
					<div class="logo"><a href="/RMF/"></a></div>
					<a class="nav-toggle" id="open-nav" href="#">&#9776;</a>
<nav>
	
	
		
                
		  <a href="/RMF/" class="">HOME</a>
                
	
	
		
                
		  <a href="/RMF/govern.html" class="">GOVERN</a>
                
	
	
		
                
		  <a href="/RMF/map.html" class="">MAP</a>
                
	
	
		
                
                  <a class="">MEASURE</a>
                
	
	
		
                
                  <a class="">MANAGE</a>
                
	
	
		
                
		  <a href="/RMF/characteristics.html" class="">CHARACTERISTICS</a>
                
	
	
		
                
		  <a href="/RMF/terms.html" class="">TERMS</a>
                
	
</nav>

				</section>
				<section class="hero_search">
					<h1>AI Risk Managment Framework</h1>
					<h1>Playbook</h1>
					<form action="/RMF/search/" method="get">
	<input type="search" name="q"  placeholder="What would you like to know?" autofocus>
	<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"/>
    <path d="M0 0h24v24H0z" fill="none"/>
</svg>
	<input type="submit" value="Search" style="display: none;">
</form>
				</section>
			</div>

		</header>
		<section class="content">
			<div class="wrapper">
				<p><span id="search-process">Loading</span> results <span id="search-query-container" style="display: none;">for "<strong id="search-query"></strong>"</span></p>
<ul id="search-results"></ul>

<script>
	window.data = {
		
			
				
					
					
					"5-impact-assessment-2012-01-16-assess-impacts-html": {
						"id": "5-impact-assessment-2012-01-16-assess-impacts-html",
						"title": "5.3 Assessment Impacts",
						"categories": "5-Impact-Assessment",
						"url": " /5-impact-assessment/2012/01/16/assess-impacts.html",
						"content": "Question\n\nHave the potential harms been assessed in relation to the likelihood of such harms occurring and the expected benefits of the system?\n\nIf so, has this assessment been conducted by independent parties, either internally or externally?\n\nHow can my organization implement this sub-category?\n\nReview all documentation, including the stated purpose and benefit of the system and the mapped harms and their associated likelihoods.\n\nCollaboratively determine which harms can be mitigated. Document mitigation plans.\n\nDocument the system’s estimated risk. Do not deploy (no-go), or decommission, the system if estimated risk surpasses organizational tolerances or thresholds. Otherwise (go), assign the system to an appropriate risk tier and oversight resources should be assigned in alignment with assessed risk.\n\nAI actors\n\n[Organizational management, AI deployment, Operators, Human factors, Domain experts]\n\nAI Transparency considerations and resources\n\nTransparency Considerations – Key Questions: MAP 5.3\n\n  To what extent do these policies foster public trust and confidence in the use of the AI system?\n  What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?\n  How has the entity identified and mitigated potential impacts of bias in the data, including inequitable or discriminatory outcomes?\n\n\nAI Transparency Resources: MAP 5.3\n\n  Datasheets for Datasets\n  GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n  “AI policies and initiatives,” in Artificial Intelligence in Society, OECD, 2019\n  Intel.gov: AI Ethics Framework for Intelligence Community  - 2020\n  Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI - 2019\n\n\nWhat is this sub-category about?\n\n\n\nThe final output of risk triage is the go/no-go decision. This decision should take into account the mapped harms from previous steps, and the organizational capacity for their mitigation. This harms mapping step should also list system benefits beyond the status quo. The go/no-go decision can be made by an independent third-party or organizational management. For higher risk systems, it is often appropriate for technical or risk executives to be involved in the approval of go/no-go decisions. The decision to deploy should not be made by AI design and development teams, whose objective judgement may be hindered by the incentive to deploy.\n\n\n\nWhere might I go to learn more?\n\n\n\nBoard of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). Retrieved on July 6, 2022 from Federal Reserve\n\nElisa Jillson. 2021. Aiming for truth, fairness, and equity in your company’s use of AI (April 19, 2021). Retrieved on July 7, 2022 from FTC\n\nSarah Spiekermann and Till Winkler. 2020. Value-based Engineering for Ethics by Design. arXiv:2004.13676. Retrieved from arXiv:2004.13676\n\nSri Krishnamurthy. Quantifying Model Risk: Issues and approaches to measure and assess model risk when building quant models. QuantUniversity, Charlestown, MA. Retrieved on July 7, 2022 from citeseerx.ist.psu.edu"
					}
					
				
			
		
			
				
					,
					
					"5-impact-assessment-2012-01-16-likelihood-analysis-html": {
						"id": "5-impact-assessment-2012-01-16-likelihood-analysis-html",
						"title": "5.2 Likelihood Analysis",
						"categories": "5-Impact-Assessment",
						"url": " /5-impact-assessment/2012/01/16/likelihood-analysis.html",
						"content": "Question\n\nHas the likelihood of each harm been evaluated?\n\nHow can my organization implement this sub-category?\n\nDefine assessment scales for measuring impact. Scales may be qualitative, such as red-amber-green (RAG), or may entail simulations or econometric approaches. Document and apply scales uniformly to different AI systems within an organization’s purview.\n\nPlan to apply impact assessments at a specific frequency at key stages in the AI lifecycle, connected to the harm a system can cause and how quickly a system changes.\n\nAssess the benefits and potential negative impacts of the specific system in relation to false positive, false negatives, true positives and true negatives (e.g.: for binary classification systems), or for numeric over- and under-prediction. (e.g.: for continuous outcomes).\n\nReview ratings for each impact listed in the assessment.\n\nDefine scales by which probability is assessed. Scales may be qualitative, such as red-amber-green (RAG), or may entail simulations or econometric approaches. Document and apply scales uniformly to different AI systems within an organization’s purview.\n\nApply the agreed upon scale to the system and assess the likelihood of harms using the scale. Document likelihood estimates.\n\nReview likelihood estimates for each listed impact in the assessment.\n\nAI actors\n\n[Organizational management, AI design, AI development, AI deployment, Impact assessment, Human factors, TEVV]\n\nAI Transparency considerations and resources\n\nTransparency Considerations – Key Questions: MAP 5.2\n\n  Which population(s) does the AI system impact?\n  What assessments has the entity conducted on data security and privacy impacts associated with the AI system?\n  Did you ensure that the AI system can be audited by independent third parties?\n\n\nAI Transparency Resources: MAP 5.2\n\n  Datasheets for Datasets\n  GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n  “AI policies and initiatives,” in Artificial Intelligence in Society, OECD, 2019\n  Intel.gov: AI Ethics Framework for Intelligence Community  - 2020\n  Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI - 2019\n\n\nWhat is this sub-category about?\n\n\n\nAfter identifying potential AI system impacts the likelihood of a given impact should be evaluated and risk triage considered. These probability estimates are then assessed and judged for go/no-go decision. If an organization deploys the system, the likelihood estimate can be used to assign appriopriate oversight resources in accordance with risk level.\n\n\n\nWhere might I go to learn more?\n\n\n\nEmilio Gómez-González and Emilia Gómez. 2020. Artificial intelligence in medicine and healthcare. Joint Research Centre (European Commission). Retrieved from op.europa.eu\n\nArtificial Intelligence Incident Database. 2022. Retrieved from Incidentdatabase"
					}
					
				
			
		
			
				
					,
					
					"5-impact-assessment-2012-01-16-identify-impacts-html": {
						"id": "5-impact-assessment-2012-01-16-identify-impacts-html",
						"title": "5.1 Identify Impacts",
						"categories": "5-Impact-Assessment",
						"url": " /5-impact-assessment/2012/01/16/identify-impacts.html",
						"content": "Question\n\nHave the broader impacts of the system been assessed in relationship to potential users, organizations or society as a whole?\n\nHow can my organization implement this sub-category?\n\nStandardize stakeholder engagement processes at the earliest stages of system formulation to identify potential impacts from the AI system on individuals, groups, communities, organizations, and society.\n\nEmploy methods such as value sensitive design (VSD) to identify baseline organizational and societal values for evaluating alignment/misalignment of system implementation and impact.\n\nIdentify transparent approaches to seek, capture, and incorporate input from system users and other key stakeholders to assist with monitoring impacts and emergent risks.\n\nIncorporate quantitiative, qualitative, and mixed methods in the assessment and documentation of potential impacts for individuals, groups, communities, organizations, and society.\n\nIdentify a team (internal or external) that is independent of AI design and development functions to assess the impact and likelihood of potential harms in relation to the expected benefits of the system.\n\nDefine impact assessment processes that incorporate socio-technical elements and methods, and plan to normalize across organizational culture.\n\nReview and refine impact assessments once completed.\n\nAI actors\n\n[Organizational management, AI design, AI development, AI deployment, Human factors, Impact assessment]\n\nAI Transparency considerations and resources\n\nTransparency Considerations – Key Questions: MAP 5.1\n\n  If it relates to people, does it unfairly advantage or disadvantage a particular social group? In what ways? How was this mitigated?\n  If it relates to other ethically protected subjects, have appropriate obligations been met? (e.g., medical data might include information collected from animals)\n  If it relates to people, could this dataset expose people to harm or legal action? (e.g., financial social or otherwise) What was done to mitigate or reduce the potential for harm?\n\n\nAI Transparency Resources: MAP 5.1\n\n  Datasheets for Datasets\n  GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n  “AI policies and initiatives,” in Artificial Intelligence in Society, OECD, 2019\n  Intel.gov: AI Ethics Framework for Intelligence Community  - 2020\n  Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI - 2019\n\n\nWhat is this sub-category about?\n\n\n\nAs AI systems are socio-technical in nature, they are likely to have implications that extend beyond the stated purpose. These broader impacts can be classified as positive, neutral, or negative. Impacts can affect individuals, groups, communities, organizations, and society, as well as the environment, or national security. Assessing potential impacts in the mapping stage provides a baseline for system monitoring, enables adapting of tradeoffs between positive and adverse impacts, and detection of emergent risks. Impact assessments also enable risk management and mitigation, and provides an opportunity to capture new value which may arise from AI system use.\n\nDifferent stakeholder groups may be aware of, or experience, benefits or harms that are unknown to internal system designers and can share their perspectives to inform an improved system design. Stakeholder feedback during system operation is also important for monitoring emergent risks that are newly introduced or amplified by AI systems, or may not be easily recognized or detected by internal technical teams.\n\n\n\nWhere might I go to learn more?\n\n\n\nSusanne Vernim, Harald Bauer, Erwin Rauch, et al. 2022. A value sensitive design approach for designing AI-based worker assistance systems in manufacturing. Procedia Comput. Sci. 200, C (2022), 505–516. DOI\n\nHarini Suresh and John V. Guttag. 2020. A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle. arXiv:1901.10002. Retrieved from arXiv:1901.10002\n\nMargarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of Imagination in AI Infused System Development and Deployment. arXiv:2011.13416. Retrieved from [arXiv:2011.13416()]https://arxiv.org/abs/2011.13416)\n\nKonstantinia Charitoudi and Andrew Blyth. A Socio-Technical Approach to Cyber Risk Management and Impact Assessment. Journal of Information Security 4, 1 (2013), 33-41. DOI:\n\nRaji, I.D., Smart, A., White, R.N., Mitchell, M., Gebru, T., Hutchinson, B., Smith-Loud, J., Theron, D., &amp; Barnes, P. (2020). Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing. Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency.\n\nEmanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, Madeleine Clare Elish, &amp; Jacob Metcalf. 2021. Assemlbing Accountability: Algorithmic Impact Assessment for the Public Interest.  Data &amp; Society. Accessed 7/14/2022 at Link\n\nAda Lovelace Institute. 2022. Algorithmic Impact Assessment: A Case Study in Healthcare. Accessed July 14, 2022. adalovelaceinstitute\n\nMicrosoft. Responsible AI Impact Assessment Template. 2022. Accessed July 14, 2022. Microsoft-RAI-Impact-Assessment-Template\n\nMicrosoft. Responsible AI Impact Assessment Guide. 2022. Accessed July 14, 2022. Microsoft-RAI-Impact-Assessment-Guide"
					}
					
				
			
		
			
				
					,
					
					"4-third-party-2012-01-16-risk-controls-for-third-party-risks-html": {
						"id": "4-third-party-2012-01-16-risk-controls-for-third-party-risks-html",
						"title": "4.2 Risk Controls for Third Party Risks",
						"categories": "4-Third-Party",
						"url": " /4-third-party/2012/01/16/risk-controls-for-third-party-risks.html",
						"content": "Question\n\nHave internal risk controls for third-party entities been documented and applied?\n\nHow can my organization implement this sub-category?\n\nSupply inventory and approval methods, such as model documentation templates and software safelists.\n\nConduct a risk review of third-party entities required for the AI system (including for data and models) related to bias, data privacy harms, and security vulnerabilities.\n\nApply standard risk controls, such as procurement, security, and data privacy controls, to all acquired third-party technologies.\n\nPrepare for a review of AI system consultants and ensure that human resources (or other appropriate functions) are aware of their involvement.\n\nAI actors\n\n[Organizational management, AI development, third-party entities, Human factors]\n\nAI Transparency considerations and resources\n\nTransparency Considerations – Key Questions: MAP 4.2\n\n  Did you ensure that the AI system can be audited by independent third parties?\n  To what extent do these policies foster public trust and confidence in the use of the AI system?\n  Did you establish mechanisms that facilitate the AI system’s auditability (e.g. traceability of the development process, the sourcing of training data and the logging of the AI system’s processes, outcomes, positive and negative impact)?\n\n\nAI Transparency Resources: MAP 4.2\n\n  GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n  Intel.gov: AI Ethics Framework for Intelligence Community  - 2020\n  WEF Model AI Governance Framework Assessment 2020\n  Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI - 2019\n\n\nWhat is this sub-category about?\n\n\n\nAI developers often access open-source, or otherwise freely available, third-party technologies. These technologies are known to have privacy, bias and security risks. Simply because a model, software, or data artifact is used commonly by AI system developers does not mean that it is free of risk.\n\nSome institutions may not apply standard procurement, human resource, or other risk controls, to third-party AI entities in the same manner as for more standard technologies. In spite of - and due to - these difficulties, the importance of maintaining internal risk controls for third-party entities cannot be overstated.\n\n\n\nWhere might I go to learn more?\n\n\n\nOffice of the Comptroller of the Currency. 2021. Comptroller’s Handbook: Model Risk Management, Version 1.0, August 2021. Retrieved on July 7, 2022 from OCC\n\n“Proposed Interagency Guidance on Third-Party Relationships: Risk Management,” 2021, available at URL"
					}
					
				
			
		
			
				
					,
					
					"4-third-party-2012-01-16-document-third-party-risks-html": {
						"id": "4-third-party-2012-01-16-document-third-party-risks-html",
						"title": "4.1 Document Third Party Risks",
						"categories": "4-Third-Party",
						"url": " /4-third-party/2012/01/16/document-third-party-risks.html",
						"content": "Question\n\nHave risks in the third-party entities been mapped, triaged and documented?\n\nHow can my organization implement this sub-category?\n\nReview audit reports, testing results, product roadmaps, warranites, terms of service, end-user license agreements, contracts and other documentation related to third-party entities that can assist in value assessment and risk management.\n\nReview third-party software release schedules and software change management plans (hotfixes, patches, updates, forward- and backward- compatibility guarantees) for irregularities.\n\nReview potential technology and human redunancies to address third-party entity failures.\n\nInventory third-party entities (hardware, open-source software, foundation models, open source data, proprietary software, proprietary data, etc.) necessary for implementation and maintenance of the system.\n\nAI actors\n\n[Organizational management, AI development, third-party entities, AI deployment, Human factors]\n\nAI Transparency considerations and resources\n\nTransparency Considerations – Key Questions: MAP 4.1\n\n  Did you establish a process for third parties (e.g. suppliers, end-users, subjects, distributors/vendors or workers) to report potential vulnerabilities, risks or biases in the AI system?\n  If your organization obtained datasets from a third party, did your organization assess and manage the risks of using such datasets?\n  How will the results independently verified?\n\n\nAI Transparency Resources: MAP 4.1\n\n  GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n  Intel.gov: AI Ethics Framework for Intelligence Community  - 2020\n  WEF Model AI Governance Framework Assessment 2020\n\n\nWhat is this sub-category about?\n\n\n\nRisks from third-party technologies and personnel are often more difficult to map, as AI technologies are particularly complex or opaque and third-party organization risk tolerances or values may not align to those of the consuming institution. In spite of - and due to - these difficulties, the importance of managing and transparently documenting the risk of third-party technologies and personnel cannot be overstated. Commercial, open source and other freely-available technologies should be screened for third-party risks.\n\nFor example, foundation models are popular with AI developers due to their ease of use for inference tasks. These models tend to rely on large uncurated web datasets, which often have undisclosed origins - raising concerns about privacy, bias, and unintended effects. Other risks may arise due to increased levels of statistical uncertainty, difficulty with reproducibility, and issues with scientific validity. Bias incidents have been reported in relation to use of these models.\n\n\n\nWhere might I go to learn more?\n\n\n\nFoundation models\n\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‘21). Association for Computing Machinery, New York, NY, USA, 610–623. Link\n\nJulia Kreutzer, Isaac Caswell, Lisa Wang, et al. 2022. Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets. Transactions of the Association for Computational Linguistics 10 (2022), 50–72.  DOI:\n\nLaura Weidinger, Jonathan Uesato, Maribeth Rauh, et al. 2022. Taxonomy of Risks posed by Language Models. In 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‘22). Association for Computing Machinery, New York, NY, USA, 214–229. Link\n\nOffice of the Comptroller of the Currency. 2021. Comptroller’s Handbook: Model Risk Management, Version 1.0, August 2021. Retrieved on July 7, 2022 from OCC\n\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, et al. 2021. On the Opportunities and Risks of Foundation Models. arXiv:2108.07258. Retrieved from arXiv:2108.07258"
					}
					
				
			
		
			
				
					,
					
					"3-benefits-2012-01-16-application-scope-html": {
						"id": "3-benefits-2012-01-16-application-scope-html",
						"title": "3.3 Application Scope",
						"categories": "3-Benefits",
						"url": " /3-benefits/2012/01/16/application-scope.html",
						"content": "Question\n\nHas the scope of the system been narrowed to reasonably ensure it will only be used as intended and that risks can be managed properly?\n\nHow can my organization implement this sub-category?\n\nDecide on narrow contexts in which a system will be deployed, such as limiting the time the system is deployed in between retrainings, targeting the geographical regions in which the system will operate, or defining clear user or stakeholder outcomes.\n\nDefine metrics by which internal and external validity, and bias in system outcomes will be measured, and establish expected thresholds or tolerances for those metrics.\n\nInvolve legal or product policy teams in developing and enforcing guidelines or policies for managing risks and misuse due to downstream deployment by third parties.\n\nAI actors\n\n[Organizational management, AI design, AI development, AI deployment, TEVV]\n\nAI Transparency considerations and resources\n\nTransparency Considerations – Key Questions: MAP 3.3\n\n  To what extent has the entity clearly defined technical specifications and requirements for the AI system?\n  How do the technical specifications and requirements align with the AI system’s goals and objectives?\n  How might you respond to an intelligence consumer asking “How do you know this?”\n\n\nAI Transparency Resources: MAP 3.3\n\n  Intel.gov: AI Ethics Framework for Intelligence Community  - 2020\n  GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n  Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI – 2019\n\n\nWhat is this sub-category about?\n\n\n\nOnce an AI system has been ideated, and its context and purpose mapped, its scope of application should be narrowed. Systems that function in a narrow scope tend to enable better mapping, measurement and management of risks, in both the learning or decision-making task, and in the system context. For example, open-ended chatbot systems that interact with the public on the internet have a large number of risks that may be difficult to map, measure and manage due to the variability from both the decision-making task and the operational context. A narrow application scope also helps ease oversight functions and related resources within an organization.\n\n\n\nWhere might I go to learn more?\n\n\n\nMark J. Van der Laan and Sherri Rose (2018). Targeted Learning in Data Science. Cham: Springer International Publishing, 2018.\n\nAlice Zheng. 2015. Evaluating Machine Learning Models (2015). O’Reilly. Retrieved from https://www.oreilly.com/library/view/evaluating-machine-learning/9781492048756/.\n\nBrenda Leong and Patrick Hall (2021). 5 things lawyers should know about artificial intelligence. ABA Journal. Retrieved from https://www.abajournal.com/columns/article/5-things-lawyers-should-know-about-artificial-intelligence."
					}
					
				
			
		
			
				
					,
					
					"3-benefits-2012-01-16-system-cost-html": {
						"id": "3-benefits-2012-01-16-system-cost-html",
						"title": "3.2 System Cost",
						"categories": "3-Benefits",
						"url": " /3-benefits/2012/01/16/system-cost.html",
						"content": "Question\n\nHave the potential negative impacts been articulated and assessed?\n\nHow can my organization implement this sub-category?\n\nPerform a context analysis to list and assess specific negative impacts arising from a lack of trustworthy characteristics, including:\n\n\n  Negative impacts are mapped to confusion matrix elements, e.g., true positives and true negative decisions leading to feedback loops and a lack of reliability, or false positives or false negatives leading to safety risks.\n  Negative impacts are mapped to numeric over- and under-prediction, e.g., a lack of robustness leads to frequent numeric under-prediction and undervaluing of assets.\n  Denigration, erasure, exnomination, misrecognition, stereotyping, underrepresenation and other content harms leading to biased or unfair outcomes are mapped.\n  If negative impacts are not direct or obvious, teams should engage in structured discussions with external stakeholders to document responses to:\n    \n      Who could be harmed?\n      What could be harmed?\n      When could harm arise?\n      How could harm arise?\n    \n  \n\n\nConduct context analysis and elicit and document stakeholder requirements using resources such as software engineering and product management guidance, user stories, user interaction/user experience (UI/UX) research; and document relevant technical and socio-technical trustworthy characteristics (such as ML-specific security vulnerabilities or recourse mechanisms for end-users).\n\nProvide robust processes and resources to AI designers and developers for enumeration of negative impacts in conjuction with system requirements, or for review or audit of harms mitigation once a system is deployed.\n\nImplement processes to evaluate qualitative and quantitative costs of internal and external failures of AI systems, with linkages to actions for prevention, detection, and/or correction of potential risks and related impacts. Evaluate failure costs to inform go/no-go decisions at all stages of the AI system lifecycle.\n\nAI actors\n\n[Organizational management, AI design, AI development, AI deployment, Human factors, Domain experts]\n\nAI Transparency considerations and resources\n\nTransparency Considerations – Key Questions: MAP 3.2\n\n  To what extent does the system/entity consistently measure progress towards stated goals and objectives?\n  To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?\n  Have you documented and explained that machine errors may differ from human errors?\n\n\nAI Transparency Resources: MAP 3.2\n\n  Intel.gov: AI Ethics Framework for Intelligence Community  - 2020\n  GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n  Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI – 2019\n\n\nWhat is this sub-category about?\n\n\n\nAnticipating the kinds of negative impacts AI systems may cause is a difficult task. System designers can work with other stakeholders to identify such impacts. These may or may not be linked to poor system performance, or may range from minor annoyance due to online content to serious injury, financial losses, or regulatory enforcement actions. Structured questions, discussions or assessments may aide system designers and other stakeholders in brainstorming efforts. Shallow assessments may result in erroneous determinations of no risk or no harm for more complex or higher risk systems. Impact assessments can help inform the determined impact of a system, and subsequently, its overall risk assessment.\n\n\n\nWhere might I go to learn more?\n\n\nAbagayle Lee Blank. 2019. Computer vision machine learning and future-oriented ethics. Honors Project. Seattle Pacific University (SPU), Seattle, WA. Available at https://digitalcommons.spu.edu/cgi/viewcontent.cgi?article=1100&amp;context=honorsprojects\n\nMargarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of Imagination in AI Infused System Development and Deployment. arXiv:2011.13416. Retrieved from https://arxiv.org/abs/2011.13416\n\nJeff Patton. 2014. User Story Mapping. O’Reilly, Sebastopol, CA. See https://www.jpattonassociates.com/story-mapping/\n\nMargarita Boenig-Liptsin, Anissa Tanweer &amp; Ari Edmundson (2022) Data Science Ethos Lifecycle: Interplay of ethical thinking and data science practice, Journal of Statistics and Data Science Education, DOI: 10.1080/26939169.2022.2089411\n\nJ. Cohen, D. S. Katz, M. Barker, N. Chue Hong, R. Haines and C. Jay, ““The Four Pillars of Research Software Engineering,”” in IEEE Software, vol. 38, no. 1, pp. 97-105, Jan.-Feb. 2021, doi: 10.1109/MS.2020.2973362.\n\nNational Academies of Sciences, Engineering, and Medicine 2022. ““Introduction”” in Fostering Responsible Computing Research: Foundations and Practices. Washington, DC: The National Academies Press. https://doi.org/10.17226/26507."
					}
					
				
			
		
			
				
					,
					
					"3-benefits-2012-01-16-system-benefits-html": {
						"id": "3-benefits-2012-01-16-system-benefits-html",
						"title": "3.1 System Benefits",
						"categories": "3-Benefits",
						"url": " /3-benefits/2012/01/16/system-benefits.html",
						"content": "Question\n\nHave the benefits of the system been articulated and assessed?\n\nHow can my organization implement this sub-category?\n\nIncorporate stakeholder feedback to document perceived system benefits above the status quo and align with other stakeholder needs; consider inclusion of a more formal cost-benefit analysis in subsequent measurement stages of risk management.\n\nPerform context analysis related to timeframe, safety concerns, geographic area, physical environment, ecosystems, social environment, and cultural norms within which the expected benefits or negative impacts exist.\n\nAI actors\n\n[Organizational management, AI design, AI development, Human factors, TEVV]\n\nAI Transparency considerations and resources\n\nTransparency Considerations – Key Questions: MAP 3.1\n\n  Did you communicate the benefits of the AI system to users?\n  Did you provide appropriate training material and disclaimers to users on how to adequately use the AI system?\n  Did your organization implement a risk management system to address risks involved in deploying the identified AI solution (e.g. personnel risk or changes to commercial objectives)?\n\n\nAI Transparency Resources: MAP 3.1\n\n  Intel.gov: AI Ethics Framework for Intelligence Community  - 2020\n  GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n  Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI – 2019\n\n\nWhat is this sub-category about?\n\n\n\nAI systems have implicit and explicit costs, and inherent risks. Systems should present a benefit to counteract these costs and risks. To identify system benefits, organizations can define and document system purpose and utility, along with foreseeable costs, risks, and negative impacts. This information should include credible justification for anticipated benefits beyond the status quo.\n\n\n\nWhere might I go to learn more?\n\n\n\nRoel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial intelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004-3702. DOI: https://doi.org/10.1016/j.artint.2021.103555\n\nSamir Passi and Solon Barocas. 2019. Problem Formulation and Fairness. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* ‘19). Association for Computing Machinery, New York, NY, USA, 39–48. https://doi.org/10.1145/3287560.3287567"
					}
					
				
			
		
			
				
					,
					
					"2-operation-2012-01-16-data-collection-and-selection-html": {
						"id": "2-operation-2012-01-16-data-collection-and-selection-html",
						"title": "2.3 Data Collection and Selection",
						"categories": "2-Operation",
						"url": " /2-operation/2012/01/16/data-collection-and-selection.html",
						"content": "Question\n\nHave the risks related to data preparation (e.g., collection, selection or labelling) been defined related to training, testing and deployment of the system?\n\nHow can my organization implement this sub-category?\n\nMap adherence to policies that address validity, bias, privacy and security in the use of data for AI systems; ensure that oversight and audit functions, and documentation processes exist.\n\nWork with subject-matter experts to incorporate awareness of how human behavior is reflected in datasets, organizational factors/dynamics, and societal dynamics; identify techniques to mitigate the sources of bias (systemic, computational, human-cognitive) in computational models and systems, and the assumptions and decisions in their development.\n\nDevelop context mapping practices, utilize documentation practices, and document assumptions and constructs used in the selection, curation, preparation, and analysis of data.\n\nPlan for data use in AI systems to ensure it is collected and selected in alignment with experimental design practices and that data used in AI systems is reasonably linked to the documented purpose of the AI system (for example, by causal discovery methods).\n\nDocument assumptions and ensure validation of concepts used in all stages of data selection or collection, cleaning, and analysis. Document known limitations and efforts to mitigate risk related to training data collection, selection, and data labeling, cleaning and analysis (e.g. treatment of missing, spurious, or outlier data; biased estimators).\n\nMap and document assumptions and decisions during problem formulation, when identifying constructs and proxy targets, and in the development of indices, especially when seeking to measure concepts that are inherently unobservable (when using constructs such as “hireability”, “criminality”, “lendability”).\n\nMap mechanisms for following rigorous experimental design, hypothesis generation, and hypothesis testing.\n\nPlan to demonstrate that the deployed product is measuring the concept it intends to measure (aka construct validity) and is generalizable beyond the conditions under which the technology was developed (aka external validity). Adhere to standard statistical principles for justifying the scope, and generalizing outcomes of AI systems. Document the extent to which the proposed technology does not meet these or other validation criteria.\n\nPlan for alignment of system requirements and implementations with data privacy and security norms and policies.\n\nConsider and document any potential negative impacts due to supply chain issues as related to organizational values.\n\nAI actors\n\n[Organizational management, AI design, AI development, AI deployment, Human factors, Domain experts]\n\nAI Transparency considerations and resources\n\nTransparency Considerations – Key Questions: MAP 2.3\n\n  Are there any known errors, sources of noise, or redundancies in the data?\n  Over what time-frame was the data collected? Does the collection time-frame match the creation time-frame\n  What is the variable selection and evaluation process?\n  How was the data collected? Who was involved in the data collection process? If the dataset relates to people (e.g., their attributes) or was generated by people, were they informed about the data collection? (e.g., datasets that collect writing, photos, interactions, transactions, etc.)\n  As time passes and conditions change, is the training data still representative of the operational environment?\n\n\nAI Transparency Resources: MAP 2.3\n\n  Datasheets for Datasets\n  WEF Model AI Governance Framework Assessment 2020\n  Companion to the Model AI Governance Framework- 2020\n  GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n  ATARC Model Transparency Assessment (WD) – 2020\n  Transparency in Artificial Intelligence - S. Larsson and F. Heintz – 2020\n\n\nWhat is this sub-category about?\n\n\n\nAI design and development practices often rely on large scale datasets to drive ML processes. This reliance may lead researchers, developers, and practitioners to select data based on availability and accessibility and adapt their questions accordingly - which can lead to downstream risks. Additionally, selected datasets and/or attributes within datasets may not provide good proxies, measures, or predictors for operationalizing the phenomenon that the AI system intends to support or inform. These factors heighten the importance of experimental design best practices and consideration of data suitability and data privacy concerns. Constructs and concepts that underlie data selection and use should be empirically validated.\n\nA clear understanding of data content (e.g., data dictionaries, datasheets), data lineage, provenance, representativeness, legal basis for use, and security is important to risk management efforts. Data may be wrong, of low quality, or otherwise inaccurate. Data used in AI development processes may not be related to or representative of populations or the phenomena that are being modeled. The data that is collected can differ significantly from what occurs in the real world. Disproportionaly impacted groups may include black, indigenous, and people of color, women, LGBTQ+ individuals, people with disabilities, and people with limited access to computer network technologies. These groups tend to be historically excluded in the large scale datasets used in AI systems. Other issues arise due to the common ML practice of reusing datasets. Under such practices, datasets may become disconnected from the social contexts and time periods of their creation. Moreover, datasets may be collected or used in contravention of data privacy norms, policies or laws. Datasets may also present security concerns, such as being poisoned by bad actors in attempt to alter systems outcomes.\n\n\n\nWhere might I go to learn more?\n\n\n\nChallenges with dataset selection\n\nAlexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre Kiciman. 2019. Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries. Front. Big Data 2, 13 (11 July 2019). DOI: Link\n\nAmandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, et al. 2020. Data and its (dis)contents: A survey of dataset development and use in machine learning research. arXiv:2012.05345. Retrieved from arXiv:2012.05345\n\nCatherine D’Ignazio and Lauren F. Klein. 2020. Data Feminism. The MIT Press, Cambridge, MA. See Link\n\nMiceli, M., &amp; Posada, J. (2022). The Data-Production Dispositif. ArXiv, abs/2205.11963.\n\nBarbara Plank. 2016. What to do about non-standard (or non-canonical) language in NLP. arXiv:1608.07836. Retrieved from arXiv:1608.07836\n\nDataset and test, evaluation, validation and verification (TEVV) processes in AI system development\n\nNational Institute of Standards and Technology (NIST), Reva Schwartz, Apostol Vassilev, et al. 2022. NIST Special Publication 1270 Towards a Standard for Identifying and Managing Bias in Artificial Intelligence. DOI: Link\n\nInioluwa Deborah Raji, Emily M. Bender, Amandalynne Paullada, et al. 2021. AI and the Everything in the Whole Wide World Benchmark. arXiv:2111.15366. Retrieved from arXiv:2111.15366\n\nStatistical balance\n\nZiad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting racial bias in an algorithm used to manage the health of populations. Science 366, 6464 (25 Oct. 2019), 447-453. DOI: Link\n\nAmandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, et al. 2020. Data and its (dis)contents: A survey of dataset development and use in machine learning research. arXiv:2012.05345. Retrieved from arXiv:2012.05345\n\nSolon Barocas, Anhong Guo, Ece Kamar, et al. 2021. Designing Disaggregated Evaluations of AI Systems: Choices, Considerations, and Tradeoffs. Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. Association for Computing Machinery, New York, NY, USA, 368–378. Link\n\nMeasurement and evaluation\n\nAbigail Z. Jacobs and Hanna Wallach. 2021. Measurement and Fairness. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‘21). Association for Computing Machinery, New York, NY, USA, 375–385. Link\n\nBen Hutchinson, Negar Rostamzadeh, Christina Greer, et al. 2022. Evaluation Gaps in Machine Learning Practice. arXiv:2205.05256. Retrieved from arXiv:2205.05256\n\nExisting frameworks\n\nNational Institute of Standards and Technology. (2018). Framework for improving critical infrastructure cybersecurity. URL: https://nvlpubs.nist.gov/nistpubs/CSWP/NIST .CSWP, 4162018.\n\nBoeckl, K. R., &amp; Lefkovitz, N. B. (2020). NIST privacy framework: A tool for improving privacy through enterprise risk management, version 1.0. URL"
					}
					
				
			
		
			
				
					,
					
					"2-operation-2012-01-16-operational-context-html": {
						"id": "2-operation-2012-01-16-operational-context-html",
						"title": "2.2 Operational Context",
						"categories": "2-Operation",
						"url": " /2-operation/2012/01/16/operational-context.html",
						"content": "Question\n\nHas the deployment environment for the system been assessed and evaluated, including how output from the system will be utilized?\n\nHow can my organization implement this sub-category?\n\nDocument system requirements to account for risks that may arise when AI systems interact within intended operating context (beyond simply describing the requirements of the learning or decision-making task), and due to human-AI configurations.\n\nFollow stakeholder feedback processes for determining whether a system achieves its documented purpose within its operating context, and whether users correctly comprehend system outputs or results.\n\nDocument any dependencies on upstream data or AI systems or instances in which the system in question is an upstream dependency for another data or AI system. Document any connection to external networks (including the internet), financial markets, critical infrastructure or other possibilities for serious negative externalities, list negative impacts, and consider the broader risk threshold and resulting go/no-go decision.\n\nAI actors\n\n[AI design, AI deployment, Human factors, Operators, Domain Experts]\n\nAI Transparency considerations and resources\n\nTransparency Considerations – Key Questions: MAP 2.2\n\n  Does the AI solution provides sufficient information to assist the personnel to make an informed decision and take actions accordingly?\n  To what extent is the output of each component appropriate for the operational context?\n  What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals     impacted by use of the AI system?\n  Based on the assessment, did your organization implement the appropriate level of human involvement in AI-augmented decision-making? (WEF Assessment)\n  How will the accountable human(s) address changes in accuracy and precision due to either an adversary’s attempts to disrupt the AI or unrelated changes in operational/business environment, which may impact the accuracy of the AI?\n\n\nAI Transparency Resources: MAP 2.2\n\n  Datasheets for Datasets\n  WEF Model AI Governance Framework Assessment 2020\n  Companion to the Model AI Governance Framework- 2020\n  ATARC Model Transparency Assessment (WD) – 2020\n  Transparency in Artificial Intelligence - S. Larsson and F. Heintz – 2020\n\n\nWhat is this sub-category about?\n\n\n\nAI systems sometimes perform poorly, or present unanticipated negative impacts, once deployed in a given operational setting. Risks and incidents, such as misinterpretation of results by system users, may arise due to a variety of factors. One factor is an inability to anticipate downstream uses, either due to lack of information about deployment context, or development under highly-controlled environments or in optimized scenarios. Teams can reduce the likelihood of such incidents through detailed consideration of stakeholder input and enhanced contextual awareness for how an AI system may interact in its real-world setting. Recommended practices include broad stakeholder engagement with potentially impacted community groups, consideration of user interaction and user experience (UI/UX) factors, and regular system testing and evaluation in non-optimized conditions.\n\n\n\nWhere might I go to learn more?\n\n\n\nSmith, C. J. (2019). Designing trustworthy AI: A human-machine teaming framework to guide development. arXiv preprint arXiv:1910.03515.\n\nWarden T, Carayon P, Roth EM, et al. The National Academies Board on Human System Integration (BOHSI) Panel: Explainable AI, System Transparency, and Human Machine Teaming. Proceedings of the Human Factors and Ergonomics Society Annual Meeting. 2019;63(1):631-635. doi:10.1177/1071181319631100"
					}
					
				
			
		
			
				
					,
					
					"2-operation-2012-01-16-learning-task-html": {
						"id": "2-operation-2012-01-16-learning-task-html",
						"title": "2.1 Learning Task",
						"categories": "2-Operation",
						"url": " /2-operation/2012/01/16/learning-task.html",
						"content": "Question\n\nHas the specific learning task of the system been defined?\n\nHow can my organization implement this sub-category?\n\nNarrowly define and document AI system learning task(s) along with known assumptions and limitations.\n\nAI actors\n\n[AI design, Human factor]\n\nAI Transparency considerations and resources\n\nTransparency Considerations – Key Questions: MAP 2.1\n\n\n  To what extent has the entity clearly defined technical specifications and requirements for the AI system?\n  To what extent has the entity documented the AI system’s development, testing methodology, metrics, and performance outcomes?\n  How do the technical specifications and requirements align with the AI system’s goals and objectives?\n  Did your organization implement accountability-based practices in data management and protection (e.g. the PDPA and OECD Privacy Principles)?\n  How are outputs marked to clearly show that they came from an AI?\n\n\nAI Transparency Resources: MAP 2.1\n\n\n  Datasheets for Datasets\n  WEF Model AI Governance Framework Assessment 2020\n  Companion to the Model AI Governance Framework- 2020\n  ATARC Model Transparency Assessment (WD) – 2020\n  Transparency in Artificial Intelligence - S. Larsson and F. Heintz – 2020\n\n\nWhat is this sub-category about?\n\n\n\nIt’s important to specifiy and clearly define the technical learning or decision-making task an AI system is designed to accomplish, and the benefits that the system will provide. The clearer and narrower the definition of the task, the easier it is to map benefits and risks, leading to more fulsome risk management for the system.\n\n\n\nWhere might I go to learn more?\n\n\n\nLeong, Brenda (2020). The Spectrum of Artificial Intelligence - An Infographic Tool. Future of Privacy Forum. Retrieved from Future of Privacy Forum\n\nBrownlee, Jason (2020). A Tour of Machine Learning Algorithms. Machine Learning Mastery. Retrieved from Machine Learning Mastery."
					}
					
				
			
		
			
				
					,
					
					"1-context-2012-01-16-system-requirements-html": {
						"id": "1-context-2012-01-16-system-requirements-html",
						"title": "1.6 System Requirements",
						"categories": "1-Context",
						"url": " /1-context/2012/01/16/system-requirements.html",
						"content": "Question\n\nHave the requirements of the system been set forth and documented?\n\nHow can my organization implement this sub-category?\n\nDocument appropriate timelines for system development.\n\nMap software planning, testing, and security procedures into AI system requirements.\n\nIdentify AI actors who will be responsible for troubleshooting, operating, and overseeing the system, document their responsibilities, and ensure they are enabled through system design and/or training to perform this role effectively.\n\nSearch past failed designs and document incidents relating to systems with similar designs.\n\nProactively incorporate trustworthy characteristics into system requirements, such as mechanisms for actionable recourse or decommission when an incident occurs or is deemed highly probable to occur.\n\nDocument feedback from external stakeholders, and evaluate and incorporate feedback into system requirements, including any decisions to ignore or downplay such input.\n\nAnalyze context to address system requirements and list specific negative impacts that may arise from a lack of trustworthy characteristics.\n\nRely on software engineering, product management, and participatory engagement techniques and procedures – such as product requirement documents (PRDs), user stories, user interaction/user experience (UI/UX) research, systems engineering, ethnography and related field methods – to elicit and document stakeholder requirements.\n\nAI actors\n\n[Organizational management, AI design, AI development, TEVV, Human factors, Operators, Domain Experts]\n\nAI Transparency considerations and resources\n\nTransparency Considerations – Key Questions: MAP 1.6\n\n  What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?\n  To what extent is this information sufficient and appropriate to promote transparency? Promote transparency by enabling external stakeholders to access information on the design, operation, and limitations of the AI system.\n  To what extent has relevant information been disclosed regarding the use of AI systems, such as (a) what the system is for, (b) what it is not for, (c) how it was designed, and (d) what its limitations are? (Documentation and external communication can offer a way for entities to provide transparency.)\n  What metrics has the entity developed to measure performance of the AI system?\n  What justifications, if any, has the entity provided for the assumptions, boundaries, and limitations of the AI system?\n\n\nAI Transparency Resources: MAP 1.6\n\n  GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n  “Stakeholders in Explainable AI,” Sep. 2018, [Online]. Available: http://arxiv.org/abs/1810.00184\n  “Including Insights from the Comptroller General’s Forum on the Oversight of Artificial Intelligence An Accountability Framework for Federal Agencies and Other Entities,” 2021\n  “HIGH-LEVEL EXPERT GROUP ON ARTIFICIAL INTELLIGENCE SET UP BY THE EUROPEAN COMMISSION ETHICS GUIDELINES FOR TRUSTWORTHY AI.” [Online]. Available: https://ec.europa.eu/digital-\n\n\nWhat is this sub-category about?\n\n\n\nAI systems are often developed under timelines that may preclude formal software requirements documentation. Without written requirements, system development may inadvertently shortcut business and stakeholder needs, and be unduly affected by implicit human biases, such as confirmation bias and groupthink.\n\nAI system requirements should address relevant trustworthy characteristics from the outset, such as accounting for ML-specific security vulnerabilities or defining transparent and actionable recourse mechanisms for end-users and operators. Shoe-horning mechanisms into a finished system, for example, to address explanation or managing bias, can be less effective than designing a system to address trustworthy charactersitics from the outset. \n\n\nWhere might I go to learn more?\n\n\n\nAmit K. Chopra, Fabiano Dalpiaz, F. Başak Aydemir, et al. 2014. Protos: Foundations for engineering innovative sociotechnical systems. In 2014 IEEE 22nd International Requirements Engineering Conference (RE) (2014), 53-62. DOI: https://doi.org/10.1109/RE.2014.6912247\n\nAndrew D. Selbst, Danah Boyd, Sorelle A. Friedler, et al. 2019. Fairness and Abstraction in Sociotechnical Systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* ‘19). Association for Computing Machinery, New York, NY, USA, 59–68. https://doi.org/10.1145/3287560.3287598\n\nGordon Baxter and Ian Sommerville. 2011. Socio-technical systems: From design methods to systems engineering. Interacting with Computers, 23, 1 (Jan. 2011), 4–17. DOI: https://doi.org/10.1016/j.intcom.2010.07.003\n\nRoel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial intelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004-3702. DOI: https://doi.org/10.1016/j.artint.2021.103555\n\nYilin Huang, Giacomo Poderi, Sanja Šćepanović, et al. 2019. Embedding Internet-of-Things in Large-Scale Socio-technical Systems: A Community-Oriented Design in Future Smart Grids. In The Internet of Things for Smart Urban Ecosystems (2019), 125-150. Springer, Cham. DOI: https://doi.org/10.1007/978-3-319-96550-5_6"
					}
					
				
			
		
			
				
					,
					
					"1-context-2012-01-16-stakeholders-html": {
						"id": "1-context-2012-01-16-stakeholders-html",
						"title": "1.5 Stakeholder Identification and Outreach",
						"categories": "1-Context",
						"url": " /1-context/2012/01/16/stakeholders.html",
						"content": "Question\n\nHave the internal and external stakeholders been defined, and is there a plan to engage or allow for continuous engagement that will inform system design and maintenance?\n\nHow can my organization implement this sub-category?\n\nDocument the groups or individuals that comprise the system’s internal and external stakeholders.\n\nDefine and standardize mechansims by which AI design and development teams can interact with internal and external stakeholders to receive feedback on system design and implementation decisions.\n\nEnsure appropriate skills and practices exist in-house for carrying out stakeholder engagement activities, eliciting, capturing and synthesizing stakeholder feedback, and translating it for design and development teams.\n\nDefine which AI actors - beyond AI design and development teams- will perform reviews of system design, implementation, and operation, and standard test, evaluation, verification, and validation practices.\n\nAI actors\n\n[Organizational management, AI design, Domain experts, Human factors]\n\nAI Transparency considerations and resources\n\nTransparency Considerations – Key Questions: MAP 1.5\n\n  To what extent has the entity clarified the roles, responsibilities, and delegated authorities to relevant stakeholders?\n  Who is responsible for checking the AI at these intervals?\n  What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?\n  If anyone believed that the AI no longer meets this ethical framework, who will be responsible for receiving the concern and as appropriate investigating and remediating the issue? Do they have authority to modify, limit, or stop the use of the AI?\n  How easily accessible and current is the information available to external stakeholders?\n\n\nAI Transparency Resources: MAP 1.5\n\n  GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n  “Stakeholders in Explainable AI,” Sep. 2018, [Online]. Available: http://arxiv.org/abs/1810.00184\n  “Including Insights from the Comptroller General’s Forum on the Oversight of Artificial Intelligence An Accountability Framework for Federal Agencies and Other Entities,” 2021\n  “HIGH-LEVEL EXPERT GROUP ON ARTIFICIAL INTELLIGENCE SET UP BY THE EUROPEAN COMMISSION ETHICS GUIDELINES FOR TRUSTWORTHY AI.” [Online]. Available: https://ec.europa.eu/digital-\n\n\nWhat is this sub-category about?\n\n\n\nAn important step to risk mitigation is regular communication with stakeholder groups. Stakeholders can help provide valuable input related to system gaps and limitations. Each organization may differ in the types and number of stakeholders with which they engage.\n\n\n\nWhere might I go to learn more?\n\n\n\nVincent T. Covello. 2021. Stakeholder Engagement and Empowerment. In Communicating in Risk, Crisis, and High Stress Situations (Vincent T. Covello, ed.), 87-109. DOI: https://doi.org/10.1002/9781119081753.ch5\n\nYilin Huang, Giacomo Poderi, Sanja Šćepanović, et al. 2019. Embedding Internet-of-Things in Large-Scale Socio-technical Systems: A Community-Oriented Design in Future Smart Grids. In The Internet of Things for Smart Urban Ecosystems (2019), 125-150. Springer, Cham. DOI: https://doi.org/10.1007/978-3-319-96550-5_6\n\nEloise Taysom and Nathan Crilly. 2017. Resilience in Sociotechnical Systems: The Perspectives of Multiple Stakeholders. She Ji: The Journal of Design, Economics, and Innovation, 3, 3 (2017), 165-182, ISSN 2405-8726. DOI: https://doi.org/10.1016/j.sheji.2017.10.011"
					}
					
				
			
		
			
				
					,
					
					"1-context-2012-01-16-risk-tolerance-html": {
						"id": "1-context-2012-01-16-risk-tolerance-html",
						"title": "1.4 Risk Tolerances",
						"categories": "1-Context",
						"url": " /1-context/2012/01/16/risk-tolerance.html",
						"content": "Question\n\nHave risk tolerances for the system been articulated?\n\nHow can my organization implement this sub-category?\n\nEstablish a maximum risk tolerance, beyond which systems are not deployed.\n\nEstablish risk tiers for AI systems, with allocated oversight resources aligned to a system’s risk tier.\n\nDo not deploy AI systems that exceed organizational risk tolerances or for “off-label” purposes.\n\nDo not apply algorithmic models or AI-based systems to tasks or within social, domain, or organizational contexts for which they were not designed. Any attempt to do so should be approached with caution, especially in settings that organizations have deemed as high-risk. Document decisionmaking steps, especially in regards to risk-related trade-offs and system limitations.\n\nAI actors\n\n[Organizational management, AI design, AI development, AI deployment, TEVV]\n\nAI Transparency considerations and resources\n\nTransparency Considerations – Key Questions: MAP 1.4\n\n  What justifications, if any, has the entity provided for the assumptions, boundaries, and limitations of the AI system?\n  How has the entity identified and mitigated potential impacts of bias in the data, including inequitable or discriminatory outcomes?\n  To what extent are the established procedures effective in mitigating bias, inequity, and other concerns resulting from the system?\n\n\nAI Transparency Resources: MAP 1.4\n\n  GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n  WEF Model AI Governance Framework Assessment 2020\n  Companion to the WEF Model AI Governance Framework- 2020\n\n\nWhat is this sub-category about?\n\n\n\nRisk tolerance reflects the level and type of risk the organization will accept while conducting its mission and carrying out its strategy. The decision to implement and deploy a proposed AI system should not be pre-ordained, and should align with broader organizational risk tolerances, often established by boards of directors, compliance-focused executives or risk functions.  For each system, a go/no-go decision is made, perhaps at multiple points during the lifecycle of a system. Such decisions are best made in consultation with, but independent of stakeholders with a vested financial or reputational interest. For higher risk systems, it is often appropriate for technical or risk executives to be involved in the approval of go/no-go decisions. An affirmative organizational decision regarding risk tolerances for AI systems should be made, evaluated, and implemented on an ongoing basis, and tied to technical, business, and oversight functions so that appropriate risk-based decisions can be made about when to design, implement, deploy, decommission, or terminate a system.\n\n\n\nWhere might I go to learn more?\n\n\n\nBoard of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). Retrieved on July 6, 2022 from https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm\n\nThe Office of the Comptroller of the Currency. Enterprise Risk Appetite Statement. (Nov. 20, 2019). Retrieved on July 12, 2022 from https://www.occ.treas.gov/publications-and-resources/publications/banker-education/files/pub-risk-appetite-statement.pdf"
					}
					
				
			
		
			
				
					,
					
					"1-context-2012-01-16-mission-goals-html": {
						"id": "1-context-2012-01-16-mission-goals-html",
						"title": "1.3 Mission Goals",
						"categories": "1-Context",
						"url": " /1-context/2012/01/16/mission-goals.html",
						"content": "Question\n\nDoes the purpose of the AI system, and its interactions with its context, align to organizations values?\n\nHow can my organization implement this sub-category?\n\nReview and/or reject documented concerns about context and the system’s purpose as related to organization’s stated values, as defined by mission statements, corporate social responsibility commitments, or AI principles. Reconsider design, implementation, or deployment of AI systems that may lead to impacts which do not reflect institutional values.\n\nAI actors\n\n[Organizational management]\n\nAI Transparency considerations and resources\n\nTransparency Considerations – Key Questions: MAP 1.3\n\n  What goals and objectives does the entity expect to achieve by designing, developing, and/or deploying the AI system?\n  To what extent are the model outputs consistent with the entity’s values and principles to foster public trust and equity?\n  To what extent are the metrics consistent with system goals, objectives, and constraints, including ethical and compliance considerations?\n\n\nAI Transparency Resources: MAP 1.3\n\n  GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n  Intel.gov: AI Ethics Framework for Intelligence Community  - 2020\n  WEF Model AI Governance Framework Assessment 2020\n  “Including Insights from the Comptroller General’s Forum on the Oversight of Artificial Intelligence An Accountability Framework for Federal Agencies and Other Entities,” 2021\n\n\nWhat is this sub-category about?\n\n\n\nSocio-technical AI risks emerge from the interplay of technical development decisions and how a system is used, who operates it, and the social context into which it is deployed.\n\n\n\nWhere might I go to learn more?\n\n\n\nAlgorithm Watch. AI Ethics Guidelines Global Inventory. Retrieved from https://inventory.algorithmwatch.org/\n\nEmanuel Moss and Jacob Metcalf. 2020. Ethics Owners: A New Model of Organizational Responsibility in Data-Driven Technology Companies. Data &amp; Society Research Institute. Retrieved from https://datasociety.net/pubs/Ethics-Owners.pdf\n\nFuture of Life Institute. Asilomar AI Principles. Retrieved from https://futureoflife.org/2017/08/11/ai-principles/\n\nLeonard Haas, Sebastian Gießler, and Veronika Thiel. 2020. In the realm of paper tigers – exploring the failings of AI ethics guidelines. (April 28, 2020). Retrieved on July 6, 2022 from https://algorithmwatch.org/en/ai-ethics-guidelines-inventory-upgrade-2020/"
					}
					
				
			
		
			
				
					,
					
					"1-context-2012-01-16-business-value-html": {
						"id": "1-context-2012-01-16-business-value-html",
						"title": "1.2 Business Value",
						"categories": "1-Context",
						"url": " /1-context/2012/01/16/business-value.html",
						"content": "Question\n\nHas the value of the system been documented (or re-evaluated, if deployed) in light of its business, or other, utility?\n\nIf reevaluated, how frequently?\n\nHow can my organization implement this sub-category?\n\nEstablish visibility mechanisms into the development of AI systems; review the documented system purpose; determine any misalignment between societal norms and values, and the organization’s stated values, code of ethics, and incentives that may contribute to harmful impacts; and identify approaches for aligning and mitigating risk.\n\nAI actors\n\n[Organizational management]\n\nAI Transparency considerations and resources\n\nTransparency Considerations – Key Questions: MAP 1.2\n\n  How does the AI system help the entity meet its goals and objectives?\n  How do the technical specifications and requirements align with the AI system’s goals and objectives?\n  To what extent is the output of each component appropriate for the operational context?\n  What (other) tasks could the dataset be used for? Are there obvious tasks for which it should not be used?\n\n\nAI Transparency Resources: MAP 1.2\n\n  Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI – 2019\n  Including Insights from the Comptroller General’s Forum on the Oversight of Artificial Intelligence An Accountability Framework for Federal Agencies and Other Entities,” 2021\n  Datasheets for Datasets”\n\n\nWhat is this sub-category about?\n\n\n\nSystems should present a business benefit beyond the status quo, because of related implicit and explicit costs and inherent risks. Defining and documenting the specific business purpose of an AI system, or other value presented by an AI system, assists in evaluating system risks and eases “go/no-go” determinations when weighing a system’s benefits against associated risks.\n\n\n\nWhere might I go to learn more?\n\n\n\nAbeba Birhane, Pratyusha Kalluri, Dallas Card, et al. 2022. The Values Encoded in Machine Learning Research. arXiv:2106.15590. Retrieved from https://arxiv.org/abs/2106.15590\n\nBoard of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). Retrieved on July 6, 2022 from https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm"
					}
					
				
			
		
			
				
					,
					
					"1-context-2012-01-16-intended-purpose-html": {
						"id": "1-context-2012-01-16-intended-purpose-html",
						"title": "1.1 Intended Purpose",
						"categories": "1-Context",
						"url": " /1-context/2012/01/16/intended-purpose.html",
						"content": "Question\n\nHave the purpose, setting, users, operators, or subjects, expectations, benefits and potential negative impacts been determined and documented?\n\nHow can my organization implement this sub-category?\n\n\n  Plan and document the composition of AI design and development teams to reflect inter-disciplinary roles, competencies, skills and capacity for AI efforts; and ensure that team membership incorporates demographic diversity and broad domain expertise.\n  Gain and maintain familiarity with the complexities and interdependencies of deployed AI systems; terminology and concepts from disciplines outside of AI practice such as the law, sociology, psychology, public policy, and systems design and engineering.\n  Maintain awareness of industry and technical standards and appropriate legal standards.\n  Track, document or inventory the organization’s AI systems, including existing systems and third-party entities associated with AI systems.\n  Gain and maintain awareness for how to scientifically evaulate claims about AI system performance and benefits before launching into system design and development to enable adherence to responsible practices.\n  Define and document the task, purpose, minimum functionality, and benefits of the AI system, and consider whether the project is worth pursuing.\n  Define the context of use, including operational environment; impacts to individuals, groups, communities, organizations, and society; user characteristics; task; and social environment; determine the user and organizational requirements, including business requirements, user requirements, and technical requirements.\n  Identify human-AI interaction and/or roles, such as whether the application will support human decision making, replace a human, and make predictions; plan for risks related to these configurations; and document requirements, roles, and responsibilities for human oversight of deployed systems.\n\n\nAI actors\n\n[Organizational management, AI design, AI development, AI deployment, Domain experts, Human factors]\n\nAI Transparency considerations and resources\n\nTransparency Considerations – Key Questions: MAP 1.1\n\n  Who is ultimately responsible for the decisions of the AI and is this person aware of the intended uses and limitations of the analytic?\n  Who will be responsible for maintaining, re-verifying, monitoring, and updating this AI once deployed?\n  Who is accountable for the ethical considerations during all stages of the AI lifecycle?\n  Why was the dataset created? (e.g., were there specific tasks in mind, or a specific gap that needed to be filled?)\n  How does the entity ensure that the data collected are adequate, relevant, and not excessive in relation to the intended purpose?\n\n\nAI Transparency Resources: MAP 1.1\n\n  Datasheets for Datasets\n  GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities\n  “Stakeholders in Explainable AI,” Sep. 2018, [Online]. link\n\n\nWhat is this sub-category about?\n\n\n\nContext includes the intended and actual setting in which it is deployed, the specific set of users, operators or subjects along with their expectations, concept of operations, intended purposes and impacts of system use, the necessary requirements to ensure the system can be optimally deployed and operated, potential negative impacts to individuals, groups, communities, organizations, and society and any other system or context specifications, or legal requirements, or impacts to the environment. Context may also include unintended, downstream, off-label, or other unforeseen scopes of application.\n\nA fundamental step to mapping context is having a broad and appropriate set of skills and perspectives at the table. Within an organization this means team composition- demographic, disciplinary, experiential- that can enhance creativity and the consideration of risks. Organizational management should recognize the importance of diversity beyond its business case. By providing license for all team members to freely engage in critical inquiry, management can work to ensure that pervasive institutional biases are not inadvertently squashing creativity. This commitment to diverse and inclusive teaming increases the ability of an organization to broaden their contextual perspectives, check their assumptions about context of use, recognize when systems are not functional within and out of the intended context, and identify constraints in real world applications that may lead to harmful impacts.\n\n\n\nWhere might I go to learn more?\n\n\n\nFor more information about:\n\nSocio-technical systems    \nAndrew D. Selbst, Danah Boyd, Sorelle A. Friedler, et al. 2019. Fairness and Abstraction in Sociotechnical Systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT’19). Association for Computing Machinery, New York, NY, USA, 59–68. link\n\nProblem formulation  \nRoel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial intelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004-3702. link\n\nSamir Passi and Solon Barocas. 2019. Problem Formulation and Fairness. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT’19). Association for Computing Machinery, New York, NY, USA, 39–48. link\n\nContext mapping  \nEmilio Gómez-González and Emilia Gómez. 2020. Artificial intelligence in medicine and healthcare. Joint Research Centre (European Commission). Retrieved from link\n\nSarah Spiekermann and Till Winkler. 2020. Value-based Engineering for Ethics by Design. arXiv:2004.13676.  Retrieved from link\n\nSocial Impact Lab. 2017. Framework for Context Analysis of Technologies in Social Change Projects (Draft v2.0). Retrieved from link\n\nSolon Barocas, Asia J. Biega, Margarita Boyarskaya, et al. 2021. Responsible computing during COVID-19 and beyond. Commun. ACM 64, 7 (July 2021), 30–32. link\n\nIdentification of harms  \nHarini Suresh and John V. Guttag. 2020. A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle. arXiv:1901.10002. Retrieved from link\n\nMargarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of Imagination in AI Infused System Development and Deployment. arXiv:2011.13416. Retrieved from link\n\nMeasurement and evaluation \nAbigail Z. Jacobs and Hanna Wallach. 2021. Measurement and Fairness. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‘21). Association for Computing Machinery, New York, NY, USA, 375–385. https://doi.org/10.1145/3442188.3445901\n\nBen Hutchinson, Negar Rostamzadeh, Christina Greer, et al. 2022. Evaluation Gaps in Machine Learning Practice. arXiv:2205.05256. Retrieved from https://arxiv.org/abs/2205.05256\n\nUnderstanding and documenting limitations in ML   \nAlexander D’Amour, Katherine Heller, Dan Moldovan, et al. 2020. Underspecification Presents Challenges for Credibility in Modern Machine Learning. arXiv:2011.03395. Retrieved from https://arxiv.org/abs/2011.03395\n\nJessie J. Smith, Saleema Amershi, Solon Barocas, et al. 2022. REAL ML: Recognizing, Exploring, and Articulating Limitations of Machine Learning Research. arXiv:2205.08363. Retrieved from https://arxiv.org/abs/2205.08363\n\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, et al. 2019. Model Cards for Model Reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* ‘19). Association for Computing Machinery, New York, NY, USA, 220–229. https://doi.org/10.1145/3287560.3287596\n\nMatthew Arnold, Rachel K. E. Bellamy, Michael Hind, et al. 2019. FactSheets: Increasing Trust in AI Services through Supplier’s Declarations of Conformity. arXiv:1808.07261. Retrieved from https://arxiv.org/abs/1808.07261\n\nMichael A. Madaio, Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach. 2020. Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI ‘20). Association for Computing Machinery, New York, NY, USA, 1–14. https://doi.org/10.1145/3313831.3376445\n\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, et al. 2021. Datasheets for Datasets. arXiv:1803.09010. Retrieved from https://arxiv.org/abs/1803.09010\n\nBender, E. M., Friedman, B. &amp; McMillan-Major, A.,  (2022). A Guide for Writing Data Statements for Natural Language Processing. University of Washington.  Accessed July 14, 2022. .https://techpolicylab.uw.edu/wp-content/uploads/2021/11/Data_Statements_Guide_V2.pdf\n\nContext of use   \nInternational Standards Organization (ISO). 2019. ISO 9241-210:2019 Ergonomics of human-system interaction — Part 210: Human-centred design for interactive systems. Retrieved from https://www.iso.org/standard/77520.html\n\nNational Institute of Standards and Technology (NIST), Mary Theofanos, Yee-Yin Choong, et al. 2017. NIST Handbook 161 Usability Handbook for Public Safety Communications: Ensuring Successful Systems for First Responders. DOI: https://doi.org/10.6028/NIST.HB.161\n\nHuman-AI teaming  \nCommittee on Human-System Integration Research Topics for the 711th Human Performance Wing of the Air Force Research Laboratory and the National Academies of Sciences, Engineering, and Medicine. 2022. Human-AI Teaming: State-of-the-Art and Research Needs. Washington, D.C. National Academies Press. DOI: https://doi.org/10.17226/26355.2022\n\nBen Green. 2021. The Flaws of Policies Requiring Human Oversight of Government Algorithms. Computer Law &amp; Security Review 45 (26 Apr. 2021). DOI: https://dx.doi.org/10.2139/ssrn.3921216\n\nBen Green and Amba Kak. 2021. The False Comfort of Human Oversight as an Antidote to A.I. Harm. (June 15, 2021). Retrieved July 6, 2022 from https://slate.com/technology/2021/06/human-oversight-artificial-intelligence-laws.html.\n\nForough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, et al. 2021. Manipulating and Measuring Model Interpretability. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI ‘21). Association for Computing Machinery, New York, NY, USA, Article 237, 1–52. https://doi.org/10.1145/3411764.3445315\n\nSusanne Gaube, Harini Suresh, Martina Raue, et al. 2021. Do as AI say: susceptibility in deployment of clinical decision-aids. npj Digital Medicine 4, Article 31 (2021). DOI: https://doi.org/10.1038/s41746-021-00385-9\n\nZana Buçinca, Maja Barbara Malaya, and Krzysztof Z. Gajos. 2021. To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making. Proc. ACM Hum.-Comput. Interact. 5, CSCW1, Article 188 (April 2021), 21 pages. https://doi.org/10.1145/3449287\n\nWhen not to deploy     \nSolon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* ‘20). Association for Computing Machinery, New York, NY, USA, 695. https://doi.org/10.1145/3351095.3375691\n\nStatistical balance  \nZiad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting racial bias in an algorithm used to manage the health of populations. Science 366, 6464 (25 Oct. 2019), 447-453. DOI: https://doi.org/10.1126/science.aax2342\n\nDiversity     \nSina Fazelpour and Maria De-Arteaga. 2022. Diversity in sociotechnical machine learning systems. Big Data &amp; Society 9, 1 (Jan. 2022). DOI: https://doi.org/10.1177%2F20539517221082027\n\nAssessment of science in AI  \nArvind Narayanan. How to recognize AI snake oil. Retrieved July 6, 2022 from https://www.cs.princeton.edu/~arvindn/talks/MIT-STS-AI-snakeoil.pdf\n\nEmily M. Bender. 2022. On NYT Magazine on AI: Resist the Urge to be Impressed. (April 17, 2022). Retrieved July 6, 2022 from https://medium.com/@emilymenonbender/on-nyt-magazine-on-ai-resist-the-urge-to-be-impressed-3d92fd9a0edd"
					}
					
				
			
		
			
				
					,
					
					"6-third-party-2012-01-16-congingencies-for-third-party-systems-html": {
						"id": "6-third-party-2012-01-16-congingencies-for-third-party-systems-html",
						"title": "6.2 Contingencies for Third Party Systems",
						"categories": "6-Third-Party",
						"url": " /6-third-party/2012/01/16/congingencies-for-third-party-systems.html",
						"content": "Question\n\nColumn C\n\nHow can my organization implement this sub-category?\n\nColumn E\n\nAI actors\n\nColumn G\n\nAI Transparency considerations and resources\n\nColumn H\n\nWhat is this sub-category about?\n\n\n\nColumn D\n\n\n\nWhere might I go to learn more?\n\n\n\nColumn F"
					}
					
				
			
		
			
				
					,
					
					"6-third-party-2012-01-16-guideline-for-third-party-systems-html": {
						"id": "6-third-party-2012-01-16-guideline-for-third-party-systems-html",
						"title": "6.1 Guidelines for Third Party Systems",
						"categories": "6-Third-Party",
						"url": " /6-third-party/2012/01/16/guideline-for-third-party-systems.html",
						"content": "Question\n\nColumn C\n\nHow can my organization implement this sub-category?\n\nColumn E\n\nAI actors\n\nColumn G\n\nAI Transparency considerations and resources\n\nColumn H\n\nWhat is this sub-category about?\n\n\n\nColumn D\n\n\n\nWhere might I go to learn more?\n\n\n\nColumn F"
					}
					
				
			
		
			
				
					,
					
					"5-impacts-2012-01-16-post-feedback-html": {
						"id": "5-impacts-2012-01-16-post-feedback-html",
						"title": "5.2 Post-feedback Actions",
						"categories": "5-Impacts",
						"url": " /5-impacts/2012/01/16/post-feedback.html",
						"content": "Question\n\nColumn C\n\nHow can my organization implement this sub-category?\n\nColumn E\n\nAI actors\n\nColumn G\n\nAI Transparency considerations and resources\n\nColumn H\n\nWhat is this sub-category about?\n\n\n\nColumn D\n\n\n\nWhere might I go to learn more?\n\n\n\nColumn F"
					}
					
				
			
		
			
				
					,
					
					"5-impacts-2012-01-16-capturing-stakeholder-feedback-html": {
						"id": "5-impacts-2012-01-16-capturing-stakeholder-feedback-html",
						"title": "5.1 Capturing Stakeholder Feedback",
						"categories": "5-Impacts",
						"url": " /5-impacts/2012/01/16/capturing-stakeholder-feedback.html",
						"content": "Question\n\nColumn C\n\nHow can my organization implement this sub-category?\n\nColumn E\n\nAI actors\n\nColumn G\n\nAI Transparency considerations and resources\n\nColumn H\n\nWhat is this sub-category about?\n\n\n\nColumn D\n\n\n\nWhere might I go to learn more?\n\n\n\nColumn F"
					}
					
				
			
		
			
				
					,
					
					"4-risk-culture-2012-01-16-team-empowerment-html": {
						"id": "4-risk-culture-2012-01-16-team-empowerment-html",
						"title": "4.2 Team Empowerment",
						"categories": "4-Risk-Culture",
						"url": " /4-risk-culture/2012/01/16/team-empowerment.html",
						"content": "Question\n\nColumn C\n\nHow can my organization implement this sub-category?\n\nColumn E\n\nAI actors\n\nColumn G\n\nAI Transparency considerations and resources\n\nColumn H\n\nWhat is this sub-category about?\n\n\n\nColumn D\n\n\n\nWhere might I go to learn more?\n\n\n\nColumn F"
					}
					
				
			
		
			
				
					,
					
					"4-risk-culture-2012-01-16-teams-and-communications-html": {
						"id": "4-risk-culture-2012-01-16-teams-and-communications-html",
						"title": "4.1 Teams and Communication",
						"categories": "4-Risk-Culture",
						"url": " /4-risk-culture/2012/01/16/teams-and-communications.html",
						"content": "Question\n\nColumn C\n\nHow can my organization implement this sub-category?\n\nColumn E\n\nAI actors\n\nColumn G\n\nAI Transparency considerations and resources\n\nColumn H\n\nWhat is this sub-category about?\n\n\n\nColumn D\n\n\n\nWhere might I go to learn more?\n\n\n\nColumn F"
					}
					
				
			
		
			
				
					,
					
					"3-workforce-2012-01-16-team-composition-html": {
						"id": "3-workforce-2012-01-16-team-composition-html",
						"title": "3.1 Team Composition",
						"categories": "3-Workforce",
						"url": " /3-workforce/2012/01/16/team-composition.html",
						"content": "Question\n\nColumn C\n\nHow can my organization implement this sub-category?\n\nColumn E\n\nAI actors\n\nColumn G\n\nAI Transparency considerations and resources\n\nColumn H\n\nWhat is this sub-category about?\n\n\n\nColumn D\n\n\n\nWhere might I go to learn more?\n\n\n\nColumn F"
					}
					
				
			
		
			
				
					,
					
					"2-accountability-2012-01-16-executive-leadership-html": {
						"id": "2-accountability-2012-01-16-executive-leadership-html",
						"title": "2.3 Executive Leadership",
						"categories": "2-Accountability",
						"url": " /2-accountability/2012/01/16/executive-leadership.html",
						"content": "Question\n\nColumn C\n\nHow can my organization implement this sub-category?\n\nColumn E\n\nAI actors\n\nColumn G\n\nAI Transparency considerations and resources\n\nColumn H\n\nWhat is this sub-category about?\n\n\n\nColumn D\n\n\n\nWhere might I go to learn more?\n\n\n\nColumn F"
					}
					
				
			
		
			
				
					,
					
					"2-accountability-2012-01-16-education-and-training-html": {
						"id": "2-accountability-2012-01-16-education-and-training-html",
						"title": "2.2 Education and Training",
						"categories": "2-Accountability",
						"url": " /2-accountability/2012/01/16/education-and-training.html",
						"content": "Question\n\nColumn C\n\nHow can my organization implement this sub-category?\n\nColumn E\n\nAI actors\n\nColumn G\n\nAI Transparency considerations and resources\n\nColumn H\n\nWhat is this sub-category about?\n\n\n\nColumn D\n\n\n\nWhere might I go to learn more?\n\n\n\nColumn F"
					}
					
				
			
		
			
				
					,
					
					"2-accountability-2012-01-16-roles-and-responsibilities-html": {
						"id": "2-accountability-2012-01-16-roles-and-responsibilities-html",
						"title": "2.1 Roles and Responsibilities",
						"categories": "2-Accountability",
						"url": " /2-accountability/2012/01/16/roles-and-responsibilities.html",
						"content": "Question\n\nColumn C\n\nHow can my organization implement this sub-category?\n\nColumn E\n\nAI actors\n\nColumn G\n\nAI Transparency considerations and resources\n\nColumn H\n\nWhat is this sub-category about?\n\n\n\nColumn D\n\n\n\nWhere might I go to learn more?\n\n\n\nColumn F"
					}
					
				
			
		
			
				
					,
					
					"1-4ps-2012-01-16-documentation-html": {
						"id": "1-4ps-2012-01-16-documentation-html",
						"title": "1.3 Documentation",
						"categories": "1-4Ps",
						"url": " /1-4ps/2012/01/16/documentation.html",
						"content": "Question\n\nColumn C\n\nHow can my organization implement this sub-category?\n\nColumn E\n\nAI actors\n\nColumn G\n\nAI Transparency considerations and resources\n\nColumn H\n\nWhat is this sub-category about?\n\n\n\nColumn D\n\n\n\nWhere might I go to learn more?\n\n\n\nColumn F"
					}
					
				
			
		
			
				
					,
					
					"1-4ps-2012-01-16-monitoring-and-review-html": {
						"id": "1-4ps-2012-01-16-monitoring-and-review-html",
						"title": "1.2 Monitoring and Review",
						"categories": "1-4Ps",
						"url": " /1-4ps/2012/01/16/monitoring-and-review.html",
						"content": "Question\n\nColumn C\n\nHow can my organization implement this sub-category?\n\nColumn E\n\nAI actors\n\nColumn G\n\nAI Transparency considerations and resources\n\nColumn H\n\nWhat is this sub-category about?\n\n\n\nColumn D\n\n\n\nWhere might I go to learn more?\n\n\n\nColumn F"
					}
					
				
			
		
			
				
					,
					
					"1-4ps-2012-01-16-trustworthiness-embedded-into-processes-html": {
						"id": "1-4ps-2012-01-16-trustworthiness-embedded-into-processes-html",
						"title": "1.1 Trustworthiness Embedded into Processes",
						"categories": "1-4Ps",
						"url": " /1-4ps/2012/01/16/trustworthiness-embedded-into-processes.html",
						"content": "Question\n\nColumn C\n\nHow can my organization implement this sub-category?\n\nColumn E\n\nAI actors\n\nColumn G\n\nAI Transparency considerations and resources\n\nColumn H\n\nWhat is this sub-category about?\n\n\n\nColumn D\n\n\n\nWhere might I go to learn more?\n\n\n\nColumn F"
					}
					
				
			
		
			
				
					,
					
					"robustness-2012-01-16-suggested-actions-html": {
						"id": "robustness-2012-01-16-suggested-actions-html",
						"title": "5.1 Suggested Actions",
						"categories": "Robustness",
						"url": " /robustness/2012/01/16/suggested-actions.html",
						"content": "Question\n\nWhat is this sub-category about?\n\nHow can my organization implement this sub-category?\n\nWhere might I go to learn more?"
					}
					
				
			
		
			
				
					,
					
					"reliability-2012-01-16-suggested-actions-html": {
						"id": "reliability-2012-01-16-suggested-actions-html",
						"title": "4.1 Suggested Actions",
						"categories": "Reliability",
						"url": " /reliability/2012/01/16/suggested-actions.html",
						"content": "Question\n\nWhat is this sub-category about?\n\nHow can my organization implement this sub-category?\n\nWhere might I go to learn more?"
					}
					
				
			
		
			
				
					,
					
					"accuracy-2012-01-16-suggested-actions-html": {
						"id": "accuracy-2012-01-16-suggested-actions-html",
						"title": "3.1 Suggested Actions",
						"categories": "Accuracy",
						"url": " /accuracy/2012/01/16/suggested-actions.html",
						"content": "Question\n\nWhat is this sub-category about?\n\nHow can my organization implement this sub-category?\n\nWhere might I go to learn more?"
					}
					
				
			
		
			
				
					,
					
					"assurance-2012-01-16-suggested-actions-html": {
						"id": "assurance-2012-01-16-suggested-actions-html",
						"title": "2.1 Suggested Actions",
						"categories": "Assurance",
						"url": " /assurance/2012/01/16/suggested-actions.html",
						"content": "Question\n\nWhat is this sub-category about?\n\nHow can my organization implement this sub-category?\n\nWhere might I go to learn more?"
					}
					
				
			
		
			
				
					,
					
					"bias-2012-01-16-references-html": {
						"id": "bias-2012-01-16-references-html",
						"title": "1.3 References",
						"categories": "Bias",
						"url": " /bias/2012/01/16/references.html",
						"content": "Question\n\nWhat is this sub-category about?\n\nHow can my organization implement this sub-category?\n\nWhere might I go to learn more?"
					}
					
				
			
		
			
				
					,
					
					"bias-2012-01-16-suggested-actions-html": {
						"id": "bias-2012-01-16-suggested-actions-html",
						"title": "1.2 Suggested Actions",
						"categories": "Bias",
						"url": " /bias/2012/01/16/suggested-actions.html",
						"content": "Question\n\nWhat is this sub-category about?\n\nHow can my organization implement this sub-category?\n\nWhere might I go to learn more?"
					}
					
				
			
		
			
				
					,
					
					"bias-2012-01-16-background-information-html": {
						"id": "bias-2012-01-16-background-information-html",
						"title": "1.1 Backgound Information",
						"categories": "Bias",
						"url": " /bias/2012/01/16/background-information.html",
						"content": "Question\n\nWhat is this sub-category about?\n\nHow can my organization implement this sub-category?\n\nWhere might I go to learn more?"
					}
					
				
			
		
	};
</script>
<script src="/RMF/js/lunr.min.js"></script>
<script src="/RMF/js/search.js"></script>
			</div>
		</section>

		<footer>
	<div class="wrapper">
		<p class="copyright">&copy; AI RMF Playbook Released July 2022. </p>
	</div>
</footer>


	</body>
</html>
