---
date: 2012-01-16
title: 2.2 Operational Context
categories:
  - 2-Operation
description: Operational Context
type: Map
order_number: 2
---

## <span style="color:black;font-weight:360;font-size:26px">Question</span>

Has the deployment environment for the system been assessed and evaluated, including how output from the system will be utilized?

## <span style="color:black;font-weight:360;font-size:26px">How can my organization implement this sub-category?</span>

Document system requirements to account for risks that may arise when AI systems interact within intended operating context (beyond simply describing the requirements of the learning or decision-making task), and due to human-AI configurations.

Follow stakeholder feedback processes for determining whether a system achieves its documented purpose within its operating context, and whether users correctly comprehend system outputs or results.

Document any dependencies on upstream data or AI systems or instances in which the system in question is an upstream dependency for another data or AI system. Document any connection to external networks (including the internet), financial markets, critical infrastructure or other possibilities for serious negative externalities, list negative impacts, and consider the broader risk threshold and resulting go/no-go decision.

## <span style="color:black;font-weight:360;font-size:26px">[AI actors](https://pages.nist.gov/RMF/terms.html)</span>

[AI design, AI deployment, Human factors, Operators, Domain Experts]

## <span style="color:black;font-weight:360;font-size:26px">AI Transparency considerations and resources</span>

**Transparency Considerations – Key Questions: MAP 2.2**
- Does the AI solution provides sufficient information to assist the personnel to make an informed decision and take actions accordingly?
- To what extent is the output of each component appropriate for the operational context?
- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals     impacted by use of the AI system?
- Based on the assessment, did your organization implement the appropriate level of human involvement in AI-augmented decision-making? (WEF Assessment)
- How will the accountable human(s) address changes in accuracy and precision due to either an adversary’s attempts to disrupt the AI or unrelated changes in operational/business environment, which may impact the accuracy of the AI?

**AI Transparency Resources: MAP 2.2**
- Datasheets for Datasets
- WEF Model AI Governance Framework Assessment 2020
- Companion to the Model AI Governance Framework- 2020
- ATARC Model Transparency Assessment (WD) – 2020
- Transparency in Artificial Intelligence - S. Larsson and F. Heintz – 2020 

## <span style="color:black;font-weight:360;font-size:26px">What is this sub-category about?</span>

<!--more-->

AI systems sometimes perform poorly, or present unanticipated negative impacts, once deployed in a given operational setting. Risks and incidents, such as misinterpretation of results by system users, may arise due to a variety of factors. One factor is an inability to anticipate downstream uses, either due to lack of information about deployment context, or development under highly-controlled environments or in optimized scenarios. Teams can reduce the likelihood of such incidents through detailed consideration of stakeholder input and enhanced contextual awareness for how an AI system may interact in its real-world setting. Recommended practices include broad stakeholder engagement with potentially impacted community groups, consideration of user interaction and user experience (UI/UX) factors, and regular system testing and evaluation in non-optimized conditions.

<!--more-->

## <span style="color:black;font-weight:360;font-size:26px">Where might I go to learn more?</span>

<!--more-->

Smith, C. J. (2019). Designing trustworthy AI: A human-machine teaming framework to guide development. arXiv preprint arXiv:1910.03515.
  
Warden T, Carayon P, Roth EM, et al. The National Academies Board on Human System Integration (BOHSI) Panel: Explainable AI, System Transparency, and Human Machine Teaming. Proceedings of the Human Factors and Ergonomics Society Annual Meeting. 2019;63(1):631-635. doi:10.1177/1071181319631100

