---
date: 2012-01-16
title: 5.3 Assessment Impacts
categories:
  - 5-Impact-Assessment
description: Assessment Impacts
type: Map
order_number: 3
---

## <span style="color:black;font-weight:360;font-size:26px">Question</span>

Have the potential harms been assessed in relation to the likelihood of such harms occurring and the expected benefits of the system? 

If so, has this assessment been conducted by independent parties, either internally or externally?

## <span style="color:black;font-weight:360;font-size:26px">How can my organization implement this sub-category?</span>

Review all documentation, including the stated purpose and benefit of the system and the mapped harms and their associated likelihoods. 

Collaboratively determine which harms can be mitigated. Document mitigation plans. 

Document the system's estimated risk. Do not deploy (no-go), or decommission, the system if estimated risk surpasses organizational tolerances or thresholds. Otherwise (go), assign the system to an appropriate risk tier and oversight resources should be assigned in alignment with assessed risk. 

## <span style="color:black;font-weight:360;font-size:26px">[AI actors](https://pages.nist.gov/RMF/terms.html)</span>

[Organizational management, AI deployment, Operators, Human factors, Domain experts]

## <span style="color:black;font-weight:360;font-size:26px">AI Transparency considerations and resources</span>

**Transparency Considerations – Key Questions: MAP 5.3**
- To what extent do these policies foster public trust and confidence in the use of the AI system?
- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?
- How has the entity identified and mitigated potential impacts of bias in the data, including inequitable or discriminatory outcomes?

**AI Transparency Resources: MAP 5.3**
- Datasheets for Datasets
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities
- “AI policies and initiatives,” in Artificial Intelligence in Society, OECD, 2019
- Intel.gov: AI Ethics Framework for Intelligence Community  - 2020
- Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI - 2019

## <span style="color:black;font-weight:360;font-size:26px">What is this sub-category about?</span>

<!--more-->

The final output of risk triage is the go/no-go decision. This decision should take into account the mapped harms from previous steps, and the organizational capacity for their mitigation. This harms mapping step should also list system benefits beyond the status quo. The go/no-go decision can be made by an independent third-party or organizational management. For higher risk systems, it is often appropriate for technical or risk executives to be involved in the approval of go/no-go decisions. The decision to deploy should not be made by AI design and development teams, whose objective judgement may be hindered by the incentive to deploy.

<!--more-->

## <span style="color:black;font-weight:360;font-size:26px">Where might I go to learn more?</span>

<!--more-->

Board of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). Retrieved on July 6, 2022 from [Federal Reserve](https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm)

Elisa Jillson. 2021. Aiming for truth, fairness, and equity in your company’s use of AI (April 19, 2021). Retrieved on July 7, 2022 from [FTC](https://www.ftc.gov/business-guidance/blog/2021/04/aiming-truth-fairness-equity-your-companys-use-ai)

Sarah Spiekermann and Till Winkler. 2020. Value-based Engineering for Ethics by Design. arXiv:2004.13676. Retrieved from [arXiv:2004.13676](https://arxiv.org/abs/2004.13676)

Sri Krishnamurthy. Quantifying Model Risk: Issues and approaches to measure and assess model risk when building quant models. QuantUniversity, Charlestown, MA. Retrieved on July 7, 2022 from [citeseerx.ist.psu.edu](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.986.5412&rep=rep1&type=pdf)
