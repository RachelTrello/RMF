---
date: 2012-01-16
title: 1.6 System Requirements
categories:
  - 1-Context
description: System Requirements
type: Map
order_number: 6
---

## <span style="color:black;font-weight:360;font-size:26px">Question</span>

Have the requirements of the system been set forth and documented?

## <span style="color:black;font-weight:360;font-size:26px">How can my organization implement this sub-category?</span>

Document appropriate timelines for system development. 

Map software planning, testing, and security procedures into AI system requirements. 

Identify AI actors who will be responsible for troubleshooting, operating, and overseeing the system, document their responsibilities, and ensure they are enabled through system design and/or training to perform this role effectively. 

Search past failed designs and document incidents relating to systems with similar designs. 

Proactively incorporate trustworthy characteristics into system requirements, such as mechanisms for actionable recourse or decommission when an incident occurs or is deemed highly probable to occur. 

Document feedback from external stakeholders, and evaluate and incorporate feedback into system requirements, including any decisions to ignore or downplay such input. 

Analyze context to address system requirements and list specific negative impacts that may arise from a lack of trustworthy characteristics. 

Rely on software engineering, product management, and participatory engagement techniques and procedures – such as product requirement documents (PRDs), user stories, user interaction/user experience (UI/UX) research, systems engineering, ethnography and related field methods – to elicit and document stakeholder requirements. 

## <span style="color:black;font-weight:360;font-size:26px">[AI actors](https://pages.nist.gov/RMF/terms.html)</span>

[Organizational management, AI design, AI development, TEVV, Human factors, Operators, Domain Experts]

## <span style="color:black;font-weight:360;font-size:26px">AI Transparency considerations and resources</span>

**Transparency Considerations – Key Questions: MAP 1.6**
- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?
- To what extent is this information sufficient and appropriate to promote transparency? Promote transparency by enabling external stakeholders to access information on the design, operation, and limitations of the AI system. 
- To what extent has relevant information been disclosed regarding the use of AI systems, such as (a) what the system is for, (b) what it is not for, (c) how it was designed, and (d) what its limitations are? (Documentation and external communication can offer a way for entities to provide transparency.)
- What metrics has the entity developed to measure performance of the AI system?
- What justifications, if any, has the entity provided for the assumptions, boundaries, and limitations of the AI system?

**AI Transparency Resources: MAP 1.6**
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities
- “Stakeholders in Explainable AI,” Sep. 2018, [Online]. Available: http://arxiv.org/abs/1810.00184
- “Including Insights from the Comptroller General’s Forum on the Oversight of Artificial Intelligence An Accountability Framework for Federal Agencies and Other Entities,” 2021
- “HIGH-LEVEL EXPERT GROUP ON ARTIFICIAL INTELLIGENCE SET UP BY THE EUROPEAN COMMISSION ETHICS GUIDELINES FOR TRUSTWORTHY AI.” [Online]. Available: https://ec.europa.eu/digital-

## <span style="color:black;font-weight:360;font-size:26px">What is this sub-category about?</span>

<!--more-->

AI systems are often developed under timelines that may preclude formal software requirements documentation. Without written requirements, system development may inadvertently shortcut business and stakeholder needs, and be unduly affected by implicit human biases, such as confirmation bias and groupthink.

AI system requirements should address relevant trustworthy characteristics from the outset, such as accounting for ML-specific security vulnerabilities or defining transparent and actionable recourse mechanisms for end-users and operators. Shoe-horning mechanisms into a finished system, for example, to address explanation or managing bias, can be less effective than designing a system to address trustworthy charactersitics from the outset. 
<!--more-->

## <span style="color:black;font-weight:360;font-size:26px">Where might I go to learn more?</span>

<!--more-->

Amit K. Chopra, Fabiano Dalpiaz, F. Başak Aydemir, et al. 2014. Protos: Foundations for engineering innovative sociotechnical systems. In 2014 IEEE 22nd International Requirements Engineering Conference (RE) (2014), 53-62. DOI: https://doi.org/10.1109/RE.2014.6912247

Andrew D. Selbst, Danah Boyd, Sorelle A. Friedler, et al. 2019. Fairness and Abstraction in Sociotechnical Systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* '19). Association for Computing Machinery, New York, NY, USA, 59–68. https://doi.org/10.1145/3287560.3287598

Gordon Baxter and Ian Sommerville. 2011. Socio-technical systems: From design methods to systems engineering. Interacting with Computers, 23, 1 (Jan. 2011), 4–17. DOI: https://doi.org/10.1016/j.intcom.2010.07.003

Roel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial intelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004-3702. DOI: https://doi.org/10.1016/j.artint.2021.103555

Yilin Huang, Giacomo Poderi, Sanja Šćepanović, et al. 2019. Embedding Internet-of-Things in Large-Scale Socio-technical Systems: A Community-Oriented Design in Future Smart Grids. In The Internet of Things for Smart Urban Ecosystems (2019), 125-150. Springer, Cham. DOI: https://doi.org/10.1007/978-3-319-96550-5_6
