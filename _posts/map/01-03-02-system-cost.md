---
date: 2012-01-16
title: 3.2 System Cost
categories:
  - 3-Benefits
description: System Cost
type: Map
order_number: 2
---

## <span style="color:black;font-weight:360;font-size:26px">Question</span>

Have the potential negative impacts been articulated and assessed?

## <span style="color:black;font-weight:360;font-size:26px">How can my organization implement this sub-category?</span>

Perform a context analysis to list and assess specific negative impacts arising from a lack of trustworthy characteristics, including:

* Negative impacts are mapped to confusion matrix elements, e.g., true positives and true negative decisions leading to feedback loops and a lack of reliability, or false positives or false negatives leading to safety risks.
* Negative impacts are mapped to numeric over- and under-prediction, e.g., a lack of robustness leads to frequent numeric under-prediction and undervaluing of assets.
* Denigration, erasure, exnomination, misrecognition, stereotyping, underrepresenation and other content harms leading to biased or unfair outcomes are mapped.
* If negative impacts are not direct or obvious, teams should engage in structured discussions with external stakeholders to document responses to:
    * Who could be harmed?
    * What could be harmed?
    * When could harm arise?
    * How could harm arise?

Conduct context analysis and elicit and document stakeholder requirements using resources such as software engineering and product management guidance, user stories, user interaction/user experience (UI/UX) research; and document relevant technical and socio-technical trustworthy characteristics (such as ML-specific security vulnerabilities or recourse mechanisms for end-users).

Provide robust processes and resources to AI designers and developers for enumeration of negative impacts in conjuction with system requirements, or for review or audit of harms mitigation once a system is deployed.

Implement processes to evaluate qualitative and quantitative costs of internal and external failures of AI systems, with linkages to actions for prevention, detection, and/or correction of potential risks and related impacts. Evaluate failure costs to inform go/no-go decisions at all stages of the AI system lifecycle.

## <span style="color:black;font-weight:360;font-size:26px">[AI actors](https://pages.nist.gov/RMF/terms.html)</span>

[Organizational management, AI design, AI development, AI deployment, Human factors, Domain experts]

## <span style="color:black;font-weight:360;font-size:26px">AI Transparency considerations and resources</span>

**Transparency Considerations – Key Questions: MAP 3.2** 
- To what extent does the system/entity consistently measure progress towards stated goals and objectives?
- To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?
- Have you documented and explained that machine errors may differ from human errors?

**AI Transparency Resources: MAP 3.2**
- Intel.gov: AI Ethics Framework for Intelligence Community  - 2020
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities
- Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI – 2019

## <span style="color:black;font-weight:360;font-size:26px">What is this sub-category about?</span>

<!--more-->

Anticipating the kinds of negative impacts AI systems may cause is a difficult task. System designers can work with other stakeholders to identify such impacts. These may or may not be linked to poor system performance, or may range from minor annoyance due to online content to serious injury, financial losses, or regulatory enforcement actions. Structured questions, discussions or assessments may aide system designers and other stakeholders in brainstorming efforts. Shallow assessments may result in erroneous determinations of no risk or no harm for more complex or higher risk systems. Impact assessments can help inform the determined impact of a system, and subsequently, its overall risk assessment.

<!--more-->

## <span style="color:black;font-weight:360;font-size:26px">Where might I go to learn more?</span>

<!--more-->
Abagayle Lee Blank. 2019. Computer vision machine learning and future-oriented ethics. Honors Project. Seattle Pacific University (SPU), Seattle, WA. Available at https://digitalcommons.spu.edu/cgi/viewcontent.cgi?article=1100&context=honorsprojects

Margarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of Imagination in AI Infused System Development and Deployment. arXiv:2011.13416. Retrieved from https://arxiv.org/abs/2011.13416

Jeff Patton. 2014. User Story Mapping. O'Reilly, Sebastopol, CA. See https://www.jpattonassociates.com/story-mapping/

Margarita Boenig-Liptsin, Anissa Tanweer & Ari Edmundson (2022) Data Science Ethos Lifecycle: Interplay of ethical thinking and data science practice, Journal of Statistics and Data Science Education, DOI: 10.1080/26939169.2022.2089411

J. Cohen, D. S. Katz, M. Barker, N. Chue Hong, R. Haines and C. Jay, ""The Four Pillars of Research Software Engineering,"" in IEEE Software, vol. 38, no. 1, pp. 97-105, Jan.-Feb. 2021, doi: 10.1109/MS.2020.2973362.

National Academies of Sciences, Engineering, and Medicine 2022. ""Introduction"" in Fostering Responsible Computing Research: Foundations and Practices. Washington, DC: The National Academies Press. https://doi.org/10.17226/26507.
